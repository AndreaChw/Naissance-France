---
title: "Determinants de la baisse des naissances en France de 1991 à 2019"
output: html_document
author:
  - name : "NDAYA NSABUA Niclette"
  - name : "FEHRI Mehdi"
  - name : "CHAHWAN Andrea"
  - name : "DELPONT Lauriane" 
---

```{r setup, include=FALSE}

library(readxl)
library(dplyr)
library(tidyr)
library(ggplot2)
library(writexl)
library(tseries)

data <- read_excel("C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/Data R Ajustée.xlsx") %>%
  rename_with(~ gsub("-", "_", .), everything()) %>%
  # Suppression de certaines variables jugées non pertinantes
  select(-fem, -sco_jeune, -pop, -parc_logement, -viedans5, -agemat, -solo, -loyers, 
         -tailleMenage, -consoalcool, -opi_surpoids, - opi_nervosite, -chomagefem,
         -inegalité_w_privée, -tx_emploifem, -wage_h, -opi_inquietude,
         -cadre_vie,-opi_guerre, -opi_violence,-confiance_menage,-opi_affaires)
numeric_cols <- setdiff(colnames(data), c("Temps", "fec"))


```

## I. Introduction
### I.1. Choix de la variable dépendante
En 2023, 678.000 enfants sont nés en France, c'est le chiffre le plus faibe depuis 1938 (hors période de guerre).

Cette baisse de la natalité est devenue un sujet central au point où Emmanuel Macron parle de la nécéssité d'un *"réarmement démographique"* pour la France. Nous avons donc décidé d'étudier cette question et de tenter de déterminer **les déterminants de cette baisse de la natalité**.

Nous nous sommes tout d'abord interessés au **taux de natalité**, c'est à dire le nombre de naissance rapporté à la population. Cependant, une première regression a mis en évidence que la majeure partie de cette variable est déterminée par des variables démographiques.

Nous avons donc décidé de prendre le **taux de fécondité**, qui correspond au nombre de naissances sur le nombre de femmes en âge de procréer.

```{r fec, echo=FALSE, warning=FALSE}
print(
  ggplot(data, aes_string(x = "Temps", y = "fec")) +
    geom_line(color = "blue") +
    labs(title = paste("Variation du taux de fécondité de 1991 à 2020"), x = "Temps", y = "fec")
)
numeric_cols <- setdiff(colnames(data), c("Temps", "fec"))

```

Nous avons sélectionné la plage de temps pour laquelle nous avions le plus de données, c'est à dire 1991-2019.
Nous avons choisi d'exclure l'année 2020 car la crise du covid a fortement impacté le taux de fécondité. 

### I.2. Les variables explicatives
Nous nous sommes d'abord concertés pour déterminer toutes les varaibles qui pourraient potentiellement expliquer ou être corrélées avec le taux de fécondité, organisées en 5 catégories :

  - Facteurs socio-culturels
  
  - Facteurs démographiques et biologiques
  
  - Facteurs économiques et politiques
  
  - Facteurs liés au travail et à l'éducation
  
  - Facteurs individuels et comportementaux

Nous avons réuni **37 variables explicatives**.

Nous avons identifié celles avec qui avaient le plus de corrélation avec les autres, afin de sélectionner les plus **pertinantes**.

![image](C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/ReprésentationMDS.jpg)

Finalement, nous avons retenues les variables suivantes :
*(mettre la présentation des variables ici mais on les a pas décidées encore)*

## II. Premier traitement de nos variables explicatives
### II.1. Interpolation linéaire des années (1991 - 2019) et création des variables décalées `lag`

Nous avons régularisé la variable temps en générant une séquence trimestrielle couvrant la période 1991-2019. Les valeurs manquantes des variables numériques ont été complétées à l’aide d’une interpolation linéaire, permettant d’estimer les données manquantes en se basant sur les observations existantes. Cette étape garantit un jeu de données uniforme sur toute la période. 

Ensuite, nous avons utilisé une fonction `lag` pour créer des variables décalées, en considérant que la natalité à une période donnée *(t)* est influencée par les politiques familiales et son propre niveau à la période précédente *(t-1)*. Cette hypothèse repose sur le fait qu’une gestation dure environ **9 mois**, ce qui implique un effet différé des facteurs explicatifs d’une année sur les naissances de l'année suivante 

```{r libraries, include=FALSE}
library(readxl)
library(tseries)
library(ggplot2)
library(dplyr)

```

```{r interpolation, include= TRUE, warning=FALSE}
# Interpolation trimestrielle et création des variables décalées (lags)
data_clean <- data %>%
  complete(Temps = seq(min(Temps), max(Temps), by = 0.25)) %>%
  mutate(across(all_of(numeric_cols),
                ~ approx(Temps[!is.na(.)], .[!is.na(.)], xout = Temps)$y)) %>%
  arrange(Temps) %>%
  mutate(across(setdiff(numeric_cols, "fec"), ~ lag(., n = 4), .names = "lag_{col}")) %>%
  drop_na(starts_with("lag_"))

# Sélectionner uniquement les colonnes nécessaires pour le modèle
data_work <- data_clean %>%
  select(Temps, fec, starts_with("lag_"))

```

### II.2. Standardisation du taux de fécondité
Afin de réduire l'autocorrélation et de pouvoir appliquer des Modèles comme *Arima* ou *GLS*, nous avions besoin de données **stationnaires**.
Nous avons donc d'abord essyer de soustraire la trend linéaire à la variable fec.

```{r détrendisation, include=TRUE, warning=FALSE}
modele_tendance <- lm(fec ~ Temps, data = data_work)
tendance_estimee <- fitted(modele_tendance)

# Dé-trending de la série fec
data_work$fec_detrend <- data_work$fec - tendance_estimee

# Test ADF sur la série détrendue
cat("\n--- Test ADF sur la série détrendue (fec_detrend) ---\n")
adf_result_detrend <- adf.test(na.omit(data_work$fec_detrend), alternative = "stationary")
print(adf_result_detrend)
```

Puisque même en soustrayant la trend, la fonction Fec n'était pas stationnaire, nous avons décidé d'effectuer une différenciation de la série fec, c'est à dire de calculer le taux de variation entre t et t-1.

```{r bla6, include=TRUE}
# 7. Différenciation de la série détrendue
data_work$fec_detrend_diff <- c(NA, diff(data_work$fec_detrend))

# Test ADF sur la série détrendue différenciée
cat("\n--- Test ADF sur la série détrendue différenciée (fec_detrend_diff) ---\n")
adf_result_detrend_diff <- adf.test(na.omit(data_work$fec_detrend_diff), alternative = "stationary")
print(adf_result_detrend_diff)
```

```{r détrend, include=FALSE}
# 8. Différenciation directe de la série brute
data_work$fec_diff <- c(NA, diff(data_work$fec))

# Test ADF sur la série brute différenciée
cat("\n--- Test ADF sur la série brute différenciée (fec_diff) ---\n")
adf_result_fec_diff <- adf.test(na.omit(data_work$fec_diff), alternative = "stationary")
print(adf_result_fec_diff)


# 9. Créer data_work_trend avec les variables nécessaires
# Inclure toutes les variables (Temps, fec, fec_detrend, fec_detrend_diff, fec_diff et création de fec_diff_relative)
data_work_trend <- data.frame(
  Temps = data_work$Temps,
  fec = data_work$fec,
  fec_detrend = data_work$fec_detrend,
  fec_detrend_diff = data_work$fec_detrend_diff,
  fec_diff = data_work$fec_diff
) %>%
  mutate(fec_var_relative = fec_diff / fec) # Ajouter fec_diff_relative

# Ajouter les variables explicatives
for (var in vars_explicatives) {
  data_work_trend[[var]] <- data_work[[var]]
}

# 10. Nettoyer le data_work_trend pour supprimer les lignes avec des NA
data_work_trend <- data_work_trend[complete.cases(data_work_trend), ]

# 11. Créer le dataframe fec_transformed (sans variables explicatives)
# On garde que Temps, fec, fec_detrend, fec_detrend_diff, fec_diff
fec_transformed <- data_work_trend[, c("Temps", "fec", "fec_detrend", "fec_detrend_diff", "fec_diff","fec_var_relative")]

```

```{r plot détrendé, include=TRUE}
# Superposer fec_detrend_diff et fec_diff sur un seul graphique avec un label en bas
plot(data_work_trend$Temps, data_work_trend$fec_detrend_diff, type = "l", col = "green",
     main = "Comparaison : Fec détrendue différenciée et Fec brute différenciée",
     xlab = "Temps", ylab = "Valeurs", lwd = 2)
```
Ainsi, nous avons un modèle avec une variable fec différenciée et donc stationnaire.

```{r blabla8, include=FALSE}
# 1. Modèle : notre variable Fec expliquée par la tendance ? 
modele_fec_vs_detrend <- lm(fec ~ fec_detrend, data = data_work_trend)
r2_fec_vs_detrend <- summary(modele_fec_vs_detrend)$r.squared
cat(sprintf("- Fec expliquée par Fec_detrend : %.2f%%\n", r2_fec_vs_detrend * 100))

# Ajuster le dataframe data_work_trend
data_work_trend <- data_work_trend %>%
  select(Temps, fec_var_relative, everything(), -fec, -fec_detrend, -fec_detrend_diff, -fec_diff) # Réorganiser et supprimer les colonnes inutiles

write_xlsx(data_work_trend, "C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/Data_Work_trend.xlsx")


rm(adf_result_detrend)
rm(adf_result_detrend_diff)
rm(adf_result_fec)
rm(adf_result_fec_diff)
rm(data_work)
rm(stationarity_results)
rm(modele_detrend_diff_vs_diff)
rm(modele_fec_vs_detrend)
rm(modele_tendance)

```

### II.3. Création d’un jeu de données sans les outliers et tableau récapitulatif 

Nous créons un nouveau jeu de données, `data_work2`, en supprimant les observations identifiées comme outliers, garantissant un ensemble de données nettoyées et prêtes pour les analyses.
En complément,un tableau récapitulatif liste les observations extrêmes avec leur période `(Temps)`, leur valeur de fécondité `(fec)`, et leurs résidus standardisés `(Std_Resid)`.

```{r bla, include=FALSE}
# Sélectionner uniquement les colonnes nécessaires pour le modèle
data_work <- data_clean %>%
  select(Temps, fec, starts_with("lag_"))

# Sauvegarder le DataFrame intermédiaire
write_xlsx(data_work, "C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/Data_Work.xlsx")
cat("Le fichier 'Data_Work.xlsx' a été sauvegardé après l'interpolation et le lag.\n")

#préparation des variables instrumentales candidates (condition nécessaire pour les pb d'endogénéités)
Variables_instrumentales <- data_work %>% select(Temps, lag_opi_depression, lag_depseniors, lag_synthé_Oiseau, lag_lt_interest_rate, 
                                                 lag_opi_famille, lag_opi_work_fem, lag_pib_hab, lag_acceuilenf, lag_opi_niveau_vie)

# Mise à jour de `data_work` en supprimant les variables instrumentales
data_work <- data_work %>% select(-c(lag_opi_depression, lag_depseniors, lag_synthé_Oiseau, lag_lt_interest_rate, lag_opi_famille, lag_opi_work_fem,
                                     lag_pib_hab, lag_acceuilenf, lag_opi_niveau_vie))

# Sauvegarder les résultats
write_xlsx(data_work, "C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/Data_Work.xlsx")
write_xlsx(Variables_instrumentales, "C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/Variables_Instrumentales.xlsx")

library(dplyr)
rm(data)
rm(data_clean)

data_work_trend <- read_xlsx("C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/Data_Work_trend.xlsx")

# Renommer 'fec_diff_relative' en 'fec'
data_work_trend <- data_work_trend %>%
  rename(fec = fec_var_relative)

# 1. Modèle standardisé (centré et réduit)
data_model_standardized <- data_work_trend %>%
  mutate(across(starts_with("lag_"), ~ scale(.)))

model_standardized <- lm(fec ~ . - Temps, data = data_model_standardized)
# Étape 2 : Calculer les résidus bruts et standardisés
residuals_standardized_df <- data.frame(
  Index = seq_len(nrow(data_work_trend)),
  Resid = residuals(model_standardized),      # Résidus bruts
  Std_Resid = rstandard(model_standardized),  # Résidus standardisés
  Temps = data_work_trend$Temps,
  fec = data_work_trend$fec
)
```

```{r résidus}
threshold_zoom <- 3
plot_residus_standardises <- ggplot(residuals_standardized_df %>% filter(abs(Std_Resid) <= threshold_zoom)) +
  geom_point(aes(x = Index, y = Std_Resid), color = "blue") +
  geom_hline(yintercept = c(-1, 1), color = "red", linetype = "dashed") +
  labs(
    title = "Résidus Standardisés (Zoomé)",
    x = "Index d'observation",
    y = "Résidus standardisés"
  ) +
  theme_minimal()

print(plot_residus_standardises)

```

``` {r outliers, include=TRUE}
# Étape 5 : Identifier les outliers
threshold <-2   # Seuil pour identifier les outliers
outliers_standardized_df <- residuals_standardized_df %>%
  filter(abs(Std_Resid) > threshold)

cat("\n### Résumé des outliers identifiés ###\n")
print(outliers_standardized_df)
# Étape 6 : Créer un nouveau DataFrame sans les outliers (`data_work2`)
data_work2 <- data_work_trend %>%
  mutate(Index = seq_len(nrow(data_work_trend))) %>%
  filter(!Index %in% outliers_standardized_df$Index) %>%
  select(-Index)
```   

## III. Tri des variables 

Nous avons traité les variables pour réduire les risques d’erreurs de spécification, de biais et d’instabilité du modèle, et améliorer la pertinence, la robustesse et la précision des estimations : 

### III.1. Détection et traitement de la colinéarité parfaite (alias)

Nous avons vérifié la présence de relations linéaires parfaites entre les variables explicatives et supprimé les variables redondantes. Sans cette étape, certains coefficients ne seraient pas identifiables, ce qui poserait un problème d’estimation. Cette vérification après le nettoyage des données est cruciale pour s’assurer qu’il n’y a pas de blocages mathématiques dans la régression, garantissant ainsi une estimation stable et une interprétation claire des coefficients.

``` {r bla8, include=FALSE}
cat("\n### Détection des alias (colinéarité parfaite) ###\n")

# Étape 1 : Ajuster le modèle global pour `data_work2`
model_alias <- lm(fec ~ ., data = data_work2 %>% select(-Temps))
```

```{r alias, include=TRUE, warning=FALSE}
# Étape 1 : Utiliser le modèle global pour identifier les alias
alias_info <- alias(model_alias)

# Étape 2 : Extraire les colinéarités parfaites
alias_matrix <- alias_info$Complete  # Matrice de colinéarités parfaites

# Vérifier si des alias existent
if (is.null(alias_matrix)) {
  cat("Aucune colinéarité parfaite détectée dans le modèle.\n")
} else {
  # Étape 4 : Identifier les variables colinéaires
  alias_pairs <- which(alias_matrix != 0, arr.ind = TRUE)
  
  # Extraire les noms des variables impliquées dans les colinéarités
  alias_summary <- data.frame(
    Variable_1 = rownames(alias_matrix)[alias_pairs[, 1]],
    Variable_2 = colnames(alias_matrix)[alias_pairs[, 2]]
  ) %>%
    distinct()
  
  # Afficher la liste des colinéarités parfaites
  cat("Colinéarités parfaites détectées :\n")
  print(alias_summary)
  
  # Étape 5 : Sauvegarder les colinéarités dans un fichier Excel
  write_xlsx(alias_summary, "C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/Colinear_Variables_Data_Work2.xlsx")
  cat("La liste des colinéarités parfaites a été sauvegardée dans 'Colinear_Variables_Data_Work2.xlsx'.\n")
}

# Étape 6 : Extraire les variables concernées par les alias
alias_vars <- rownames(alias_info$Complete)
cat("Variables impliquées dans les alias avec l'intercept :\n")
print(alias_vars)

# Étape 7 : Supprimer les variables alias et créer un nouveau DataFrame `data_work3`
data_work3 <- data_work2 %>%
  select(-all_of(alias_vars))

# Étape 8 : Sauvegarder le DataFrame mis à jour
write_xlsx(data_work3, "C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/Data_Work3.xlsx")
cat("Le nouveau DataFrame sans alias a été sauvegardé sous 'Data_Work3.xlsx'.\n")

# Étape 9 : Résumer les variables supprimées
removed_variables_summary <- data.frame(Variables_Supprimées = alias_vars)
```
```{r, bla11, include=FALSE}
rm(data_work2)
rm(alias_info)
rm(model_alias)
rm(removed_variables_summary)
rm(alias_matrix)
rm(alias_pairs)
rm(alias_summary)
```
### III.2.	Analyse de corrélation entre variables explicatives (matrice de corrélation, heatmap, MDS)

Nous avons ensuite calculé la matrice de corrélation et visualisé les relations entre variables à l’aide de *heatmaps et de MDS*. Cela permet de détecter la multicolinéarité non parfaite, qui, bien que moins évidente, peut fragiliser l’estimation. Cette étape approfondit l’analyse de la redondance entre les variables et aide à affiner la sélection des variables pour le modèle.

```{r bla12, include=FALSE}
library(dplyr)
library(tidyr)
library(writexl)
library(ggplot2)
library(reshape2)
library(stats)
library(igraph)

# Charger le DataFrame `data_work3` directement
data_expl <- data_work3 %>%
  select(-Temps, -fec)  # Exclure les colonnes non explicatives
```

```{r matrice de corr, include=TRUE}
# Calculer la matrice de corrélation
cor_mat <- cor(data_expl, use = "complete.obs")

# Heatmap
correlation_melted <- melt(cor_mat)
heatmap_plot <- ggplot(correlation_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0,
                       limit = c(-1, 1), space = "Lab", name = "Corrélation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Matrice de corrélation (data_work3)", x = "Variables", y = "Variables")
```
Nous sommes passés de la matrice de corrélation suivante :
![matrice de correlation avec toutes les variables]("C:/Users/PC/Downloads/Correlation_Heatmap.png")

A la matrice de corrélation suivante, une fois le tri des variables effectué :
```{r print mattrice, include=TRUE}
print(heatmap_plot)
```

![image](C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/ReprésentationMDS.jpg)

Nous avons aussi identifié les paires les plus corrélées (les résultats que vous voyez ici, sont les rasultats avec seulement nos 13 variables finales):
```{r corrélations, include=TRUE}
# Identification des paires de variables fortement corrélées
corr_threshold <- 0.5
high_corr_pairs <- correlation_melted %>%
  filter(value > corr_threshold & Var1 != Var2) %>%
  arrange(desc(value)) %>%
  mutate(pair = paste0(pmin(as.character(Var1), as.character(Var2)), "-", pmax(as.character(Var1), as.character(Var2)))) %>%
  distinct(pair, .keep_all = TRUE) %>%
  select(-pair)

cat("Paires de variables avec corrélation >", corr_threshold, ":\n")
print(high_corr_pairs)

# Compter les variables les plus fréquentes dans les paires corrélées
variable_counts <- high_corr_pairs %>%
  select(Var1, Var2) %>%
  pivot_longer(cols = everything(), values_to = "Variable") %>%
  group_by(Variable) %>%
  summarise(Frequency = n()) %>%
  arrange(desc(Frequency))

cat("\n### Variables les plus corrélées ###\n")
print(variable_counts)
```

 Nous avons ensuite cherché à visualiser la structure des variables.

```
### MDS (Multi-Dimensional Scaling)
mds_res <- cmdscale(dist_mat, k = 2)
mds_df <- data.frame(
  Dim1 = mds_res[,1],
  Dim2 = mds_res[,2],
  Variable = rownames(mds_res)
)

mds_plot <- ggplot(mds_df, aes(x = Dim1, y = Dim2, label = Variable)) +
  geom_point() +
  geom_text(vjust = -0.5, size = 3) +
  labs(title = "Représentation MDS des variables (data_work3)", x = "Dimension 1", y = "Dimension 2") +
  theme_minimal()
```

Notre **graphique MDS** ressemblait à l'origine à ceci :

![image](C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/ReprésentationMDS.jpg)

Après un tri des variables, nous avons obtenu celui-ci :

![MDS variables finales]("C:/Users/PC/Documents/Cours/Master/S1/Econométrie TD/Projet/MDS2.jpg")

Nous avons aussi créé un **graphique de réseau** ainsi qu'un **dendrogramme**.
```{r graphe de réseau, include=FALSE}
### Graphe de réseau
high_corr <- melt(cor_mat) %>%
  filter(value > corr_threshold & Var1 != Var2) %>%
  mutate(pair = paste(pmin(as.character(Var1), as.character(Var2)),
                      pmax(as.character(Var1), as.character(Var2)), sep = "-")) %>%
  distinct(pair, .keep_all = TRUE) %>%
  select(-pair)

g <- graph_from_data_frame(high_corr[, c("Var1", "Var2")], directed = FALSE)

plot(g,
     layout = layout_with_fr(g),
     vertex.size = 5,
     vertex.label.cex = 0.5,
     main = paste("Graphe de réseau des variables (corr >", corr_threshold, ")"))
cat("Graphe de réseau affiché.\n")

rm(g)
rm(hc)
rm(high_corr)
rm(high_corr_pairs)
rm(mds_df)
rm(mds_res)
rm(variable_counts)
rm(data_expl)
rm(cor_mat)
rm(correlation_melted)
rm(data_work_trend)
rm(fec_transformed)
rm(alias_summary)

```

### III.4.	Réduction des variables à fort VIF (Facteur d’inflation de la variance) et création du DataFrame final

Ensuite, nous calculons le Facteur d’inflation de la variance *(VIF)* pour chaque variable afin d’identifier et d’éliminer celles présentant une multicolinéarité excessive. En effet, un VIF élevé indique que l’estimation du coefficient est instable.Cette étape, effectuée après l’analyse de la corrélation, vise à réduire les redondances et à garantir un jeu de variables final, stable et fiable.

```
# Charger les bibliothèques nécessaires
library(car)  # Pour le calcul du VIF
library(dplyr)
library(writexl)

data_work3 <- read_xlsx("//Users/mehdifehri/Desktop/R/Code Final Fec/Data_Work3.xlsx")

cat("\n### Début de la détection des VIF élevés dans data_work3 ###\n")

# Identifier les variables explicatives
variables_explicatives <- setdiff(names(data_work3), c("fec", "Temps"))

# Créer une copie des données pour le processus VIF
data_for_vif <- data_work3

# Définir le seuil de VIF
vif_threshold <- 666

# Initialiser les listes pour stocker les résultats
iteration_results <- list()
removed_variables <- data.frame(Iteration = integer(), Variable = character(), VIF_Value = numeric(), stringsAsFactors = FALSE)

# Boucle itérative pour supprimer les variables avec VIF élevé
iteration <- 1
while (TRUE) {
  cat("\n--- Itération", iteration, "---\n")
  
  # Ajuster un modèle linéaire avec les variables explicatives restantes
  current_model <- lm(fec ~ ., data = data_for_vif[, c("fec", variables_explicatives)])
  
  # Calculer le VIF pour chaque variable explicative
  vif_values <- vif(current_model)
  
  # Afficher les VIF actuels
  cat("Facteurs d'inflation de la variance (VIF) actuels :\n")
  print(vif_values)
  
  # Sauvegarder les résultats de l'itération
  iteration_results[[iteration]] <- data.frame(Variable = names(vif_values), VIF = vif_values, Iteration = iteration)
  
  # Identifier les variables avec un VIF supérieur au seuil
  high_vif_vars <- names(vif_values[vif_values > vif_threshold])
  
  # Vérifier s'il reste des variables avec un VIF élevé
  if (length(high_vif_vars) == 0) {
    cat("Toutes les variables ont un VIF <= ", vif_threshold, ". Fin de la boucle.\n")
    break
  }
  
  # Identifier la variable avec le VIF maximum
  variable_to_remove <- high_vif_vars[which.max(vif_values[high_vif_vars])]
  max_vif_value <- max(vif_values[high_vif_vars])
  cat("Variable avec le VIF le plus élevé :", variable_to_remove, "(", max_vif_value, ")\n")
  
  # Ajouter la variable supprimée à la liste des variables supprimées
  removed_variables <- rbind(removed_variables, data.frame(Iteration = iteration, Variable = variable_to_remove, VIF_Value = max_vif_value))
  
  # Supprimer cette variable des données et des variables explicatives
  data_for_vif <- data_for_vif %>% select(-all_of(variable_to_remove))
  variables_explicatives <- setdiff(variables_explicatives, variable_to_remove)
  
  # Augmenter le compteur d'itérations
  iteration <- iteration + 1
}

data_work4 <- data_for_vif

# Sauvegarder les données finales sans VIF élevé
write_xlsx(data_work4, "//Users/mehdifehri/Desktop/R/Code Final Fec/Data_Work4.xlsx")
cat("Les données finales après suppression des variables avec VIF > ", vif_threshold, " ont été sauvegardées sous 'Data_Work4.xlsx'.\n")


# Imprimer les noms des colonnes du nouveau DataFrame `data_work4`
cat("\nNoms des colonnes du DataFrame `data_work4` :\n")
print(names(data_for_vif))

# Sauvegarder le DataFrame
write_xlsx(data_work4, "//Users/mehdifehri/Desktop/R/Code Final Fec/Data_work4.xlsx")
cat("Le fichier a été sauvegardé avec succès.\n")



rm(current_model)
rm(data_work3)
rm(iteration_results)
rm(removed_variables)
rm(data_for_vif)

```


### III.5.	Tests de spécification (RESET), choix de la forme fonctionnelle

Par la suite, nous avons réalisé le *test RESET* pour vérifier la forme fonctionnelle du modèle, en nous assurant qu’il était bien linéaire ou s’il manquait des non-linéarités. Cette vérification intervient après avoir nettoyé et stabilisé les données pour s’assurer que la spécification du modèle est correcte et que les coefficients ne sont pas biaisés. Si nécessaire, nous avons ajusté la forme fonctionnelle pour améliorer la qualité du modèle.


```
library(readxl)
library(dplyr)
library(lmtest)
library(writexl)

# Charger les données
data_work4 <- read_xlsx("//Users/mehdifehri/Desktop/R/Code Final Fec/Data_Work4.xlsx")

# Modèle OLS de base
model_ols <- lm(fec ~ . - Temps, data = data_work4)
print(summary(model_ols))

# Création d'un modèle auxiliaire avec variables explicatives log
# Transformer les variables explicatives en log avec un préfixe et nettoyer les non-log
data_work4_log <- data_work4 %>%
  mutate(across(-c(Temps, fec), log, .names = "log_{.col}")) %>%
  select(Temps, fec, starts_with("log_"))
write_xlsx(data_work4_log, "//Users/mehdifehri/Desktop/R/Code Final Fec/Data_Work4_Log.xlsx")

# Modèle OLS avec les variables explicatives transformées en log
model_ols_log <- lm(fec ~ . - Temps, data = data_work4_log)
print(summary(model_ols_log))

# Test RESET pour le modèle OLS (base)
cat("\n### Test RESET pour le modèle OLS ###\n")
reset_test_ols <- resettest(model_ols, power = 2:3, type = "fitted")
print(reset_test_ols)

if (reset_test_ols$p.value > 0.05) {
  cat("\nInterprétation : Le modèle de base est correctement spécifié. (p-value =", reset_test_ols$p.value, ")\n")
} else {
  cat("\nInterprétation : Le modèle de base est mal spécifié. (p-value =", reset_test_ols$p.value, ")\n")
}

# Test RESET pour le modèle OLS log-transformé
cat("\n### Test RESET pour le modèle OLS (log-transformé) ###\n")
reset_test_ols_log <- resettest(model_ols_log, power = 2:3, type = "fitted")
print(reset_test_ols_log)

if (reset_test_ols_log$p.value > 0.05) {
  cat("\nInterprétation : Le modèle log-transformé est correctement spécifié. (p-value =", reset_test_ols_log$p.value, ")\n")
} else {
  cat("\nInterprétation : Le modèle log-transformé est mal spécifié. (p-value =", reset_test_ols_log$p.value, ")\n")
}

```

### III.6.	Choix de la meilleure transformation : Transformation logarithmique des variables explicatives et régression OLS (interprétation double log ?)  

Une fois la forme fonctionnelle validée, nous avons opté pour une transformation logarithmique des variables explicatives et estimé un modèle en double-log. Cette transformation est utile si les relations sont multiplicatives, car elle stabilise la variance et facilite l’interprétation en termes d’élasticités. Cela rend le modèle plus stable et cohérent économiquement.

```
# Initialisation du tableau récapitulatif
model_comparison <- data.frame(
  Model = character(),
  Adjusted_R2 = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  RESET_p_value = numeric(),
  RESET_Interpretation = character(),
  stringsAsFactors = FALSE
)

# Remplir le tableau pour chaque modèle
models <- list(
  "Model OLS" = model_ols,
  "Model OLS (Log)" = model_ols_log
)

# Fonction pour obtenir l'interprétation du test RESET
perform_reset <- function(model) {
  reset_results <- resettest(model, power = 2:3, type = "fitted")
  if (reset_results$p.value > 0.05) {
    interpretation <- "Correctement spécifié"
  } else {
    interpretation <- "Mal spécifié"
  }
  return(list(p_value = reset_results$p.value, interpretation = interpretation))
}

# Remplir le tableau avec les résultats pour chaque modèle
for (name in names(models)) {
  model <- models[[name]]
  summary_model <- summary(model)
  reset_results <- perform_reset(model)
  
  model_comparison <- rbind(model_comparison, data.frame(
    Model = name,
    Adjusted_R2 = summary_model$adj.r.squared,
    AIC = AIC(model),
    BIC = BIC(model),
    RESET_p_value = reset_results$p_value,
    RESET_Interpretation = reset_results$interpretation
  ))
}

# Afficher le tableau comparatif des modèles
print(model_comparison)

# Sauvegarder le tableau comparatif dans un fichier Excel
write_xlsx(model_comparison, "//Users/mehdifehri/Desktop/R/Code Final Fec/Model_Comparison.xlsx")

rm(model)
rm(model_comparison)
rm(models)
rm(summary_model)
rm(model_ols)
rm(model_ols_log)
rm(reset_results)
rm(reset_test_ols)
rm(reset_test_ols_log)

# Charger les bibliothèques nécessaires
library(broom)
library(writexl)
library(dplyr)
library(readxl)
```

## IV. Première regression
### A. Résultats
### B. Tests des hypothèses de Gauss-Markov

## V. Deuxième regression
### A. Détrendisation
### B. GLS

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
